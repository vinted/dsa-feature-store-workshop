{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e111499a-a00c-40b1-abac-1f943dc8975b",
   "metadata": {},
   "source": [
    "# Feature Store for Data Science and Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e83ef-e776-4302-922e-9e6d1c5bf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Google Meeting and Record!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e477c5-a83a-473d-8b24-b3a852106dd3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Use GCP Vertex AI Notebooks. If that doesn't work you can use Google Colab as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40038142-be92-4274-8137-6b5949c2a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -q mamba -n base -c conda-forge -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4c81b-6f6b-4bc5-b482-d82c3290774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no (officially supported) way to install feast using conda.\n",
    "# This open issue asks for conda support: https://github.com/feast-dev/feast/issues/2748\n",
    "\n",
    "!pip install -q feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79bdd5-5902-4d66-889d-bdea0c30842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use only these direct dependencies.\n",
    "\n",
    "!mamba install -q -c conda-forge numpy pandas scikit-learn kaggle auto-sklearn -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa4084-c365-426b-a055-b6f01757d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -q -c conda-forge black nb_black -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df615ac-70fe-4039-abac-fed8c2ddea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting your code can be done automatically even in a notebook.\n",
    "# Don't waste your time doing that yourself or wrose not doing it at all.\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea0b27-38af-4acb-adf1-6fae2f71bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes there is some weird issue with autosklearn\n",
    "# Just restart the kernel and it should be fine\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import autosklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9bba7-bc13-4d84-b056-b795516916df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autoreload extension allows us to reload imported code without reloading the Jupyter lab.\n",
    "# You can read more about it here: https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce232f-2bea-4118-862f-1e2076d24d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "FEATURE_STORE_DIR = \"feature_store\"\n",
    "MODELS_DIR = \"models\"\n",
    "N_CORES = 4\n",
    "USABLE_MEMORY_PER_CORE = 1024 * 2\n",
    "RANDOM_SEED = 420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7394b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to this website and accept the rules of the competition: https://www.kaggle.com/competitions/spaceship-titanic/rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2eb254-4a3c-4c82-aa70-c036d3dd41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your kaggle.json file to the current working directory.\n",
    "# This script will place it in the right place and download the dataset.\n",
    "# P.S. if you don't have kaggle.json, you can get it using these instructions: https://github.com/Kaggle/kaggle-api#api-credentials\n",
    "\n",
    "!rm -rf ~/.kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!mv ./kaggle.json ~/.kaggle\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c spaceship-titanic\n",
    "!unzip spaceship-titanic.zip -d {DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b0b63-c066-457c-b24a-812a934b89af",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "In this section we will do the exploratory data analysis.\n",
    "What is exploratory data analysis? \n",
    "If you'd like to learn more about EDA, I'd recommend reading \"Making Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining, 2nd Edition\" by Glenn J. Myatt and Wayne P. Johnson. Or the \"Exploratory Data Analysis\" by John Tukey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d432e-6b90-4c89-bd3b-a6eb09464dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(Path(DATA_DIR) / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d080f3b-30c6-4651-a5ef-9ec259fea976",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36dec81-a130-4dcd-892c-7d9641c86a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"Transported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe3fd3-f4f5-45e6-8839-94c34f9bdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(\n",
    "    filter(\n",
    "        lambda x: x not in [\"PassengerId\", \"Cabin\", \"Name\", target_name],\n",
    "        all_data.columns,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895aae90-3807-4347-8e4e-e0e200cb9668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't like filters and lambdas you can use list comprehensions\n",
    "\n",
    "feature_names = [\n",
    "    x\n",
    "    for x in all_data.columns\n",
    "    if x not in [\"PassengerId\", \"Cabin\", \"Name\", target_name]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b8cb8-4796-402d-aa0c-536552a4f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just making sure that it worked\n",
    "\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb491e1-b02c-43f5-a5c7-f3388a8d2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split doesn't make a copy of the dataframe, but just points to the relevant sections of the original dataframe.\n",
    "# We don't have too much data, so we can make a copy.\n",
    "# It will be more convenient than overriding.\n",
    "\n",
    "train_data, val_data = train_test_split(all_data.copy(), random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764793a0-9efe-4d4a-a1cf-e19c5df8727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the data types of our data\n",
    "\n",
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb69639-10cd-4c32-bc46-878ce4e20abe",
   "metadata": {},
   "source": [
    "### Handling categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de36dc-370c-4126-94af-70581ed40a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all features are be useful for the model.\n",
    "# First let's select the categorical features that are useful.\n",
    "\n",
    "categorical_cols = [\"HomePlanet\", \"Destination\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd1092-7e9f-42b3-972d-214eb7b0b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's very important to remember the mappings.\n",
    "# Otherwise it will be impossible to reverse the encodings.\n",
    "\n",
    "category_mappings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629416c-24cf-4b39-bb3b-d4a3f54266a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Categorical is the right tool to encode data.\n",
    "# It shows the strings to the users and gives numerical data to the ML models.\n",
    "\n",
    "for col in categorical_cols:\n",
    "    train_data[col] = pd.Categorical(train_data[col])\n",
    "    category_mappings[col] = dict(enumerate(train_data[col].cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6b5e5-d872-4c99-b7db-0f4cff970496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this is how to save this category_mappings for the future.\n",
    "\n",
    "with open(Path(DATA_DIR) / \"category_mappings.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(category_mappings, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc0de1-aa31-4dc4-bcaa-ccb8628d9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the newly created mappings to convert the val_data.\n",
    "\n",
    "for col in categorical_cols:\n",
    "    val_data[col] = pd.Categorical(\n",
    "        val_data[col], categories=category_mappings[col].values()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a4c4fc-a076-4f5c-8b3d-dc38f1020a5b",
   "metadata": {},
   "source": [
    "### Handling binary features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e19d5-fb08-4a36-bedc-2ff770fd6650",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = [\"CryoSleep\", \"VIP\", \"Transported\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81e67a-c68a-45a4-8f85-199fa1bfef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to save any info about binary features.\n",
    "\n",
    "for col in binary_cols:\n",
    "    train_data[col] = train_data[col].astype(bool)\n",
    "    val_data[col] = val_data[col].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e1744-e567-4a2a-9a8a-054a8fe082b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data should be ready for training.\n",
    "# Let's take a look.\n",
    "\n",
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f47685-acbf-4a76-a410-84d5e1c9145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf753a0-e505-4879-9e3c-c8006f4dd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5799905-dd1b-4621-a7a2-a1996d162363",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae97628-65b9-453e-b1df-5d3cd0404b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: get the highest F1 macro score.\n",
    "# You only have 5 minutes to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39a56a-3050-4495-b7ef-3deb260d191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoSklearnClassifier(\n",
    "    time_left_for_this_task=60 * 2,\n",
    "    memory_limit=USABLE_MEMORY_PER_CORE,\n",
    "    n_jobs=N_CORES,\n",
    "    metric=autosklearn.metrics.roc_auc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6024ea-b0a8-4b5d-bb70-823a532cd244",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_data[feature_names],\n",
    "    train_data[target_name],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783de20-919d-4838-be6b-daaffb055c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(MODELS_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bcc281-1737-45e8-a5e5-2dca51cb227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(MODELS_DIR) / \"model1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf047275-304e-4f5b-a5db-5a4baa920688",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(MODELS_DIR) / \"model1.pkl\", \"rb\") as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856c365-5f7b-46bd-bef9-72cc20c9de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = model.predict(val_data[feature_names], n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2402a-940f-4e0f-89f9-ba6e18b56125",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(val_data[[target_name]], pred_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386167b7-d222-4dac-990a-16e8d9810d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A reasonable expected performance\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        False       0.83      0.75      0.79      1065\n",
    "#         True       0.78      0.85      0.82      1109\n",
    "\n",
    "#     accuracy                           0.80      2174\n",
    "#    macro avg       0.81      0.80      0.80      2174\n",
    "# weighted avg       0.81      0.80      0.80      2174"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90acb446-4359-42b9-97aa-4950a2606f8d",
   "metadata": {},
   "source": [
    "## Feature Store\n",
    "\n",
    "Feature store allows you to store and later retrieve features.\n",
    "Most importantly it allows you to get features for both training and (real-time) inference.\n",
    "\n",
    "### Which features should stored be in the feature store?\n",
    "\n",
    "Derived features. Doing aggregations and other transformations in backend is difficult or sometimes even impossible. Feature store is the perfect place for these types of features.\n",
    "\n",
    "Common or shareable features. Put any features that are going to be useful for many projects or for future iterations of your project.\n",
    "\n",
    "### What are the limitations of Vinted's feature store?\n",
    "\n",
    "1-24 hour delay between when the event took place and the feature depending on that event is available to use (for now). This is because of how our data warehouse jobs are scheduled. There are 1 or 24 hour jobs.\n",
    "\n",
    "Feature engineering has to be done in the data warehouse jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b9b7b-6534-4fb0-a4f8-40bde062a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: what value does the feature store bring to you?\n",
    "# A1: real-time derived features\n",
    "# A2: prepared shareable features for training and inference\n",
    "# A3: feature self-service - you won't depend on the backend engineers to create and send you features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c01177-4547-48fa-aca4-212923b8824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure that the feature store directory is free and initialize the feature store.\n",
    "\n",
    "!rm -rf {FEATURE_STORE_DIR}\n",
    "!feast init {FEATURE_STORE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2cdca-eb12-47e5-84d5-ee4ab9c74ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feast doesn't support CSV files, so we need to convert our CSV files to Parquet\n",
    "# https://github.com/feast-dev/feast/issues/2563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8334e4a-2758-40ce-97bb-a27688d455d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition, Parquet supports categorical encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e58fc9-fd6d-4b49-a10d-cfa3937ed4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only store some data in the feature store.\n",
    "# This will replicate a typical work scenario, where only some of the features are available in the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994efa3-3134-422e-8834-2093871dbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_feature_names = [\n",
    "    \"HomePlanet\",\n",
    "    \"CryoSleep\",\n",
    "    \"Destination\",\n",
    "    \"Age\",\n",
    "    \"VIP\",\n",
    "    \"Transported\",\n",
    "]\n",
    "request_feature_names = [name for name in feature_names if name not in fs_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf0982-fa68-4988-ae7e-d900db54cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca28469-7a8a-49e5-ac54-a2ba4346f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_feast = all_data.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    data_for_feast[col] = pd.Categorical(\n",
    "        data_for_feast[col], categories=category_mappings[col].values()\n",
    "    )\n",
    "\n",
    "for col in binary_cols:\n",
    "    data_for_feast[col] = data_for_feast[col].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abba421-6c77-418a-b6e0-1fab2f790ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feast also needs an event timestamp column named \"event_timestamp\" to keep track of the updates\n",
    "# https://github.com/feast-dev/feast/issues/2257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b862238-fc0c-433b-a430-989842b75015",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_feast[\"event_timestamp\"] = np.datetime64(\"2022-04-20\")\n",
    "data_for_feast.drop(request_feature_names, axis=1, inplace=True)\n",
    "data_for_feast.to_parquet(\"/home/jupyter/data/train.parquet\")\n",
    "del data_for_feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98732a-82db-4c9c-ae81-753526e8f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Replace the contents of the feature_store/example.py with the contents of example.py from the workshop repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de829c-097a-4e67-a6ab-51a6a4c06c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feast tries to read all Python files in the directory.\n",
    "# Since you've opened the feature store dict Jupyter created \".ipynb_checkpoints\" folder in there.\n",
    "# It will cause issues, so you need to remove it by running:\n",
    "\n",
    "!rm -rf feature_store/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68033f8c-019a-415b-8a4d-147ab30e5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feast apply will create (or update) a feature store deployment\n",
    "\n",
    "!cd feature_store && feast apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb255b1-e93f-48ea-b8e7-2e0e207dde0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d66777-0553-4fc7-90f0-ed4b4ddecbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will assume that \"partial_data\" will contain features that are NOT going to be in the feature store\n",
    "# Q: Where will they come from?\n",
    "# A: The request to your model.\n",
    "\n",
    "partial_data = all_data[[\"PassengerId\", *request_feature_names]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c4abb-4d5c-4257-ae30-648025171c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_data[\"event_timestamp\"] = np.datetime64(\"2022-04-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326dc7b-c5ee-459c-8038-fdb9deb86ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = FeatureStore(repo_path=FEATURE_STORE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a737b-0a7a-4d09-bf55-a054617a2f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we merge two datasets, which we will use for training.\n",
    "# \"combined_data\" is data coming from data warehouse, or any other source\n",
    "# Then we are merging it with data from the feature store using \"PassengerId\" as key\n",
    "\n",
    "combined_data = store.get_historical_features(\n",
    "    entity_df=partial_data,\n",
    "    features=[f\"space_titanic:{name}\" for name in fs_feature_names],\n",
    ").to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae63e9-7c45-4440-8633-d16968751c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that the categorical features still have the right data types.\n",
    "# Namely some of the features have to be categorical and bool.\n",
    "\n",
    "combined_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317a305-b649-4e31-837e-08758fea47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(combined_data, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ac587-b9d0-4627-a322-d98305109284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't write large classes in Jupyter, it is not the right tool for the job.\n",
    "# Just write it as a module and load it in Jupyter.\n",
    "\n",
    "from not_a_real_trainer import NotARealTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e6848-4a1e-45ec-8c2f-e19e4e55a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Martynas: to use the feature store in real-life you'll have to do the model schema changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbf378-64ac-4015-adc2-720d39ccfc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NotARealTrainer(\n",
    "    name=\"model2\",\n",
    "    work_dir=Path(MODELS_DIR),\n",
    "    memory=USABLE_MEMORY_PER_CORE,\n",
    "    time_limit_in_seconds=60 * 2,\n",
    "    fs_feature_names=fs_feature_names,\n",
    "    metric=autosklearn.metrics.roc_auc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a715d4-269e-451b-95ec-73ea30a2540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit((train_data[feature_names], train_data[target_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644efd4-0603-4a3b-9129-65af7426ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is materialization? It just means load your features to the online feature store to make them available for inference.\n",
    "\n",
    "!cd {FEATURE_STORE_DIR} && feast materialize-incremental $(date -u +\"%Y-%m-%dT%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13f7da-4861-4e39-8b36-e6a02b1973bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simulate how we'd do a real inference by calling our model repeatedly with a batch of data.\n",
    "\n",
    "%%time\n",
    "step = 1000\n",
    "predictions = []\n",
    "for i in range(0, len(val_data), step):\n",
    "    predictions.append(\n",
    "        trainer.predict(store,\n",
    "            val_data[[\"PassengerId\", *request_feature_names]].iloc[i : i + step]\n",
    "        )\n",
    "    )\n",
    "predictions = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956db57a-0fa8-4ed2-86a9-d4136b5d0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should get similar results as before.\n",
    "\n",
    "print(classification_report(val_data[[target_name]], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c4a30-46e7-4ca0-94ab-3670f4f60572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can read more about how to use the Feature Store for your projects here: https://github.com/vinted/vmip-docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfda731-36e4-4e94-9f98-7c6747ce20b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "63bfb19704d0d16ec51998aaeb8ca43d99426a2e7a5eaf7341ec9ca87fcd2911"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
